{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcdaea0a05644a8877e3089753cdf73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from attribution import patching_effect, Submodule\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer, JumpReluAutoEncoder\n",
    "from dictionary_learning.dictionary import IdentityDict\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_loading_utils import load_saes_and_submodules\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from transformers import BitsAndBytesConfig\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "from tqdm import tqdm\n",
    "from typing import Literal\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "\n",
    "DEBUGGING = False\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "# model hyperparameters\n",
    "DTYPE = t.bfloat16\n",
    "DEVICE = 'cuda:0'\n",
    "# model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE, dispatch=True)\n",
    "model = LanguageModel('google/gemma-2-2b', device_map=DEVICE, dispatch=True,\n",
    "                      attn_implementation=\"eager\", torch_dtype=DTYPE)\n",
    "activation_dim = 2304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset hyperparameters\n",
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "profession_dict = {'professor' : 21, 'nurse' : 13}\n",
    "male_prof = 'professor'\n",
    "female_prof = 'nurse'\n",
    "\n",
    "# data preparation hyperparameters\n",
    "SEED = 42\n",
    "\n",
    "def get_text_batches(\n",
    "    split: Literal[\"train\", \"test\"] = \"train\",\n",
    "    ambiguous=True, \n",
    "    batch_size=32, \n",
    "    seed=SEED\n",
    "):\n",
    "    data = dataset[split]\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg), len(pos)])\n",
    "        neg, pos = neg[:n], pos[:n]\n",
    "        data = neg + pos\n",
    "        labels = [0]*n + [1]*n\n",
    "        idxs = list(range(2*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, labels = [data[i] for i in idxs], [labels[i] for i in idxs]\n",
    "        true_labels = spurious_labels = labels\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg_neg), len(neg_pos), len(pos_neg), len(pos_pos)])\n",
    "        neg_neg, neg_pos, pos_neg, pos_pos = neg_neg[:n], neg_pos[:n], pos_neg[:n], pos_pos[:n]\n",
    "        data = neg_neg + neg_pos + pos_neg + pos_pos\n",
    "        true_labels     = [0]*n + [0]*n + [1]*n + [1]*n\n",
    "        spurious_labels = [0]*n + [1]*n + [0]*n + [1]*n\n",
    "        idxs = list(range(4*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, true_labels, spurious_labels = [data[i] for i in idxs], [true_labels[i] for i in idxs], [spurious_labels[i] for i in idxs]\n",
    "\n",
    "    batches = [\n",
    "        (data[i:i+batch_size], t.tensor(true_labels[i:i+batch_size], device=DEVICE), t.tensor(spurious_labels[i:i+batch_size], device=DEVICE)) for i in range(0, len(data), batch_size)\n",
    "    ]\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_subgroups(\n",
    "        split: Literal[\"train\", \"test\"] = \"test\",\n",
    "        batch_size=32,\n",
    "):\n",
    "    data = dataset[split]\n",
    "    neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "    neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "    pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "    pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "    neg_neg_labels, neg_pos_labels, pos_neg_labels, pos_pos_labels = (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "    subgroups = [(neg_neg, neg_neg_labels), (neg_pos, neg_pos_labels), (pos_neg, pos_neg_labels), (pos_pos, pos_pos_labels)]\n",
    "    \n",
    "    out = {}\n",
    "    for data, label_profile in subgroups:\n",
    "        out[label_profile] = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            text = data[i:i+batch_size]\n",
    "            out[label_profile].append(\n",
    "                (\n",
    "                    text,\n",
    "                    t.tensor([label_profile[0]]*len(text), device=DEVICE),\n",
    "                    t.tensor([label_profile[1]]*len(text), device=DEVICE)\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_acts(acts, attn_mask):\n",
    "    return (acts * attn_mask[:, :, None]).sum(1) / attn_mask.sum(1)[:, None]\n",
    "\n",
    "@t.no_grad()\n",
    "def collect_activations(\n",
    "    model,\n",
    "    layer,\n",
    "    text_batches,\n",
    "):\n",
    "    with tqdm(total=len(text_batches), desc=\"Collecting activations\") as pbar:\n",
    "        for text_batch, *labels in text_batches:\n",
    "            with model.trace(text_batch, **tracer_kwargs):\n",
    "                attn_mask = model.input[1]['attention_mask']\n",
    "                acts = model.model.layers[layer].output[0]\n",
    "                pooled_acts = pool_acts(acts, attn_mask).save()\n",
    "            yield pooled_acts.value, *labels\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe training hyperparameters\n",
    "\n",
    "layer = 22 # model layer for attaching linear classification head\n",
    "\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim, dtype=DTYPE):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True, dtype=dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def train_probe(\n",
    "    activation_batches,\n",
    "    label_idx=0,\n",
    "    lr=1e-2,\n",
    "    epochs=1,\n",
    "    seed=SEED,\n",
    "):\n",
    "    t.manual_seed(seed)\n",
    "\n",
    "    probe = Probe(activation_dim).to(DEVICE)\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for act, *labels, in activation_batches:\n",
    "            optimizer.zero_grad()\n",
    "            logits = probe(act)\n",
    "            loss = criterion(logits, labels[label_idx].to(logits))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "    return probe, losses\n",
    "\n",
    "@t.no_grad()\n",
    "def test_probe(\n",
    "    probe,\n",
    "    activation_batches,\n",
    "):\n",
    "    corrects = defaultdict(list)\n",
    "    for acts, *labels in activation_batches:\n",
    "        logits = probe(acts)\n",
    "        preds = (logits > 0.0).long()\n",
    "        for idx, label in enumerate(labels):\n",
    "            corrects[idx].append(preds == label)\n",
    "\n",
    "    accs = {idx: t.cat(corrects[idx]).float().mean().item() for idx in corrects}\n",
    "\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle, _ = train_probe(\n",
    "    activation_batches=collect_activations(model, layer, get_text_batches(split=\"train\", ambiguous=False))\n",
    ")\n",
    "\n",
    "ambiguous_accs = test_probe(oracle, activation_batches=collect_activations(model, layer, get_text_batches(split=\"test\", ambiguous=True)))\n",
    "print(f\"ambiguous test accuracy: {ambiguous_accs[0]}\")\n",
    "\n",
    "unambiguous_accs = test_probe(oracle, activation_batches=collect_activations(model, layer, get_text_batches(split=\"test\", ambiguous=False)))\n",
    "print(f\"ground truth accuracy: {unambiguous_accs[0]}\")\n",
    "print(f\"unintended feature accuracy: {unambiguous_accs[1]}\")\n",
    "\n",
    "for subgroup, batches in get_subgroups().items():\n",
    "    subgroup_accs = test_probe(oracle, activation_batches=collect_activations(model, layer, batches))\n",
    "    print(f\"subgroup {subgroup} accuracy: {subgroup_accs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading probe from probe_layer_22_bfloat16.pt\n"
     ]
    }
   ],
   "source": [
    "save_path = f\"probe_layer_{layer}_{str(DTYPE).split('.')[-1]}.pt\"\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"loading probe from {save_path}\")\n",
    "    probe = t.load(save_path)\n",
    "else:\n",
    "    probe, _ = train_probe(\n",
    "        activation_batches=collect_activations(model, layer, get_text_batches(split=\"train\", ambiguous=True))\n",
    "    )\n",
    "    t.save(probe, save_path)\n",
    "    print(f\"probe saved to {save_path}\")\n",
    "\n",
    "# ambiguous_accs = test_probe(probe, activation_batches=collect_activations(model, layer, get_text_batches(split=\"test\", ambiguous=True)))\n",
    "# print(f\"ambiguous test accuracy: {ambiguous_accs[0]}\")\n",
    "\n",
    "# unambiguous_accs = test_probe(probe, activation_batches=collect_activations(model, layer, get_text_batches(split=\"test\", ambiguous=False)))\n",
    "# print(f\"ground truth accuracy: {unambiguous_accs[0]}\")\n",
    "# print(f\"unintended feature accuracy: {unambiguous_accs[1]}\")\n",
    "\n",
    "# for subgroup, batches in get_subgroups().items():\n",
    "#     subgroup_accs = test_probe(probe, activation_batches=collect_activations(model, layer, batches))\n",
    "#     print(f\"subgroup {subgroup} accuracy: {subgroup_accs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Gemma SAEs:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Gemma SAEs: 100%|██████████| 23/23 [01:17<00:00,  3.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# loading dictionaries\n",
    "submodules, dictionaries = load_saes_and_submodules(\n",
    "    model, \n",
    "    \"google/gemma-2-2b\", \n",
    "    thru_layer=layer,\n",
    "    include_embed=False,\n",
    "    dtype=DTYPE,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "def metric_fn(model, labels=None):\n",
    "    attn_mask = model.input[1]['attention_mask']\n",
    "    acts = model.model.layers[layer].output[0]\n",
    "    acts = pool_acts(acts, attn_mask)\n",
    "    \n",
    "    return t.where(\n",
    "        labels == 0,\n",
    "        probe(acts),\n",
    "        - probe(acts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 100/100 [19:51<00:00, 11.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# find most influential features\n",
    "# n_batches = 200\n",
    "n_batches = 100\n",
    "batch_size = 1\n",
    "\n",
    "nodes = None\n",
    "\n",
    "for batch_idx, (clean, labels, _) in tqdm(enumerate(get_text_batches(split=\"train\", ambiguous=True, batch_size=batch_size)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    # if effects are already cached, skip\n",
    "    hash_input = clean + [s.name for s in submodules]\n",
    "    hash_str = ''.join(hash_input)\n",
    "    hash_digest = hashlib.md5(hash_str.encode()).hexdigest()\n",
    "    if os.path.exists(f\"effects/{hash_digest}.pt\"):\n",
    "        continue\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    to_save = {\n",
    "        k.name : v.detach().to(\"cpu\") for k, v in effects.items()\n",
    "    }\n",
    "    t.save(to_save, f\"effects/{hash_digest}.pt\")\n",
    "\n",
    "    del effects, _\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate effects\n",
    "aggregated_effects = {submodule.name : 0 for submodule in submodules}\n",
    "\n",
    "for idx, (clean, *_) in enumerate(get_text_batches(split=\"train\", ambiguous=True, batch_size=batch_size)):\n",
    "    if idx == n_batches:\n",
    "        break\n",
    "    hash_input = clean + [s.name for s in submodules]\n",
    "    hash_str = ''.join(hash_input)\n",
    "    hash_digest = hashlib.md5(hash_str.encode()).hexdigest()\n",
    "    effects = t.load(f\"effects/{hash_digest}.pt\")\n",
    "    for submodule in submodules:\n",
    "        aggregated_effects[submodule.name] += (\n",
    "            effects[submodule.name].act[:,1:,:] # remove BOS features\n",
    "        ).sum(dim=1).mean(dim=0)\n",
    "\n",
    "aggregated_effects = {k : v / (batch_size * n_batches) for k, v in aggregated_effects.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"attn_0\": [\n",
      "],\n",
      "\"mlp_0\": [\n",
      "],\n",
      "\"resid_0\": [\n",
      "    4449 : 7.4375,\n",
      "],\n",
      "\"attn_1\": [\n",
      "],\n",
      "\"mlp_1\": [\n",
      "],\n",
      "\"resid_1\": [\n",
      "    4521 : 7.25,\n",
      "    11782 : 6.375,\n",
      "],\n",
      "\"attn_2\": [\n",
      "],\n",
      "\"mlp_2\": [\n",
      "],\n",
      "\"resid_2\": [\n",
      "    5853 : 6.71875,\n",
      "],\n",
      "\"attn_3\": [\n",
      "],\n",
      "\"mlp_3\": [\n",
      "],\n",
      "\"resid_3\": [\n",
      "],\n",
      "\"attn_4\": [\n",
      "],\n",
      "\"mlp_4\": [\n",
      "],\n",
      "\"resid_4\": [\n",
      "],\n",
      "\"attn_5\": [\n",
      "],\n",
      "\"mlp_5\": [\n",
      "],\n",
      "\"resid_5\": [\n",
      "    2864 : 9.25,\n",
      "    11682 : 8.1875,\n",
      "],\n",
      "\"attn_6\": [\n",
      "],\n",
      "\"mlp_6\": [\n",
      "],\n",
      "\"resid_6\": [\n",
      "    4068 : 13.0625,\n",
      "    7008 : 7.1875,\n",
      "],\n",
      "\"attn_7\": [\n",
      "],\n",
      "\"mlp_7\": [\n",
      "],\n",
      "\"resid_7\": [\n",
      "    7111 : 8.0625,\n",
      "],\n",
      "\"attn_8\": [\n",
      "],\n",
      "\"mlp_8\": [\n",
      "],\n",
      "\"resid_8\": [\n",
      "    6952 : 7.09375,\n",
      "    9949 : 19.5,\n",
      "],\n",
      "\"attn_9\": [\n",
      "],\n",
      "\"mlp_9\": [\n",
      "],\n",
      "\"resid_9\": [\n",
      "    6952 : 9.375,\n",
      "    15246 : 14.875,\n",
      "],\n",
      "\"attn_10\": [\n",
      "],\n",
      "\"mlp_10\": [\n",
      "],\n",
      "\"resid_10\": [\n",
      "    3711 : 14.5625,\n",
      "    6952 : 9.125,\n",
      "],\n",
      "\"attn_11\": [\n",
      "],\n",
      "\"mlp_11\": [\n",
      "],\n",
      "\"resid_11\": [\n",
      "    3013 : 27.375,\n",
      "    4467 : 7.3125,\n",
      "    11649 : 6.15625,\n",
      "],\n",
      "\"attn_12\": [\n",
      "],\n",
      "\"mlp_12\": [\n",
      "],\n",
      "\"resid_12\": [\n",
      "    6335 : 9.25,\n",
      "    11114 : 36.25,\n",
      "    11480 : 6.28125,\n",
      "],\n",
      "\"attn_13\": [\n",
      "],\n",
      "\"mlp_13\": [\n",
      "],\n",
      "\"resid_13\": [\n",
      "    192 : 34.5,\n",
      "    495 : 6.09375,\n",
      "    14755 : 6.90625,\n",
      "],\n",
      "\"attn_14\": [\n",
      "],\n",
      "\"mlp_14\": [\n",
      "],\n",
      "\"resid_14\": [\n",
      "    2354 : 30.375,\n",
      "    12665 : 6.46875,\n",
      "],\n",
      "\"attn_15\": [\n",
      "],\n",
      "\"mlp_15\": [\n",
      "],\n",
      "\"resid_15\": [\n",
      "    798 : 25.875,\n",
      "    6211 : 6.6875,\n",
      "],\n",
      "\"attn_16\": [\n",
      "],\n",
      "\"mlp_16\": [\n",
      "],\n",
      "\"resid_16\": [\n",
      "    6047 : 6.40625,\n",
      "    15567 : 6.40625,\n",
      "    16351 : 26.25,\n",
      "],\n",
      "\"attn_17\": [\n",
      "],\n",
      "\"mlp_17\": [\n",
      "],\n",
      "\"resid_17\": [\n",
      "    6011 : 19.125,\n",
      "],\n",
      "\"attn_18\": [\n",
      "],\n",
      "\"mlp_18\": [\n",
      "],\n",
      "\"resid_18\": [\n",
      "    61 : 15.75,\n",
      "],\n",
      "\"attn_19\": [\n",
      "],\n",
      "\"mlp_19\": [\n",
      "],\n",
      "\"resid_19\": [\n",
      "    13002 : 10.5,\n",
      "],\n",
      "\"attn_20\": [\n",
      "    9711 : 8.0,\n",
      "],\n",
      "\"mlp_20\": [\n",
      "],\n",
      "\"resid_20\": [\n",
      "    7116 : 9.25,\n",
      "    7324 : 6.1875,\n",
      "    12545 : 16.5,\n",
      "],\n",
      "\"attn_21\": [\n",
      "    2740 : 11.6875,\n",
      "    10118 : 7.84375,\n",
      "],\n",
      "\"mlp_21\": [\n",
      "],\n",
      "\"resid_21\": [\n",
      "    1065 : 12.875,\n",
      "    1653 : 6.46875,\n",
      "    4430 : 24.0,\n",
      "],\n",
      "\"attn_22\": [\n",
      "],\n",
      "\"mlp_22\": [\n",
      "],\n",
      "\"resid_22\": [\n",
      "    1208 : 14.8125,\n",
      "    3497 : 24.375,\n",
      "    7961 : 7.0,\n",
      "],\n",
      "total features: 46\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in aggregated_effects.items():\n",
    "    print(f\"\\\"{k}\\\": [\")\n",
    "    for idx in (v > 6).nonzero():\n",
    "        count += 1\n",
    "        print(f\"    {idx.item()} : {v[idx].item()},\")\n",
    "    print(\"],\")\n",
    "print(f\"total features: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"600\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/21-gemmascope-att-16k/10118?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f2825bc5990>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interpret features with Neuronpedia API\n",
    "\n",
    "submodule_name = \"attn_21\"\n",
    "feature_idx = 10118\n",
    "\n",
    "from IPython.display import IFrame\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release=\"gemma-2-2b\", sae_id=None, feature_idx=feature_idx):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)\n",
    "\n",
    "# Extract the type and number from submodule_name\n",
    "submodule_type, submodule_number = submodule_name.split('_')\n",
    "\n",
    "# Construct the sae_id based on the type\n",
    "if submodule_type == \"resid\":\n",
    "    sae_id = f\"{submodule_number}-gemmascope-res-16k\"\n",
    "elif submodule_type == \"attn\":\n",
    "    sae_id = f\"{submodule_number}-gemmascope-att-16k\"\n",
    "elif submodule_type == \"mlp\":\n",
    "    sae_id = f\"{submodule_number}-gemmascope-mlp-16k\"\n",
    "else:\n",
    "    raise ValueError(\"Unknown submodule type\")\n",
    "\n",
    "html = get_dashboard_html(sae_release=\"gemma-2-2b\", sae_id=sae_id, feature_idx=feature_idx)\n",
    "IFrame(html, width=1200, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features to ablate: 43\n"
     ]
    }
   ],
   "source": [
    "feats_to_ablate = {\n",
    "    \"attn_0\": [\n",
    "    ],\n",
    "    \"mlp_0\": [\n",
    "    ],\n",
    "    \"resid_0\": [\n",
    "        4449, # \"he\", \"him\"\n",
    "    ],\n",
    "    \"attn_1\": [\n",
    "    ],\n",
    "    \"mlp_1\": [\n",
    "    ],\n",
    "    \"resid_1\": [\n",
    "        4521, # his\"\n",
    "        11782, # # \"woman\", \"she\", \"her\", \"female\"\n",
    "    ],\n",
    "    \"attn_2\": [\n",
    "    ],\n",
    "    \"mlp_2\": [\n",
    "    ],\n",
    "    \"resid_2\": [\n",
    "        5853, # \"he\"\n",
    "    ],\n",
    "    \"attn_3\": [\n",
    "    ],\n",
    "    \"mlp_3\": [\n",
    "    ],\n",
    "    \"resid_3\": [\n",
    "    ],\n",
    "    \"attn_4\": [\n",
    "    ],\n",
    "    \"mlp_4\": [\n",
    "    ],\n",
    "    \"resid_4\": [\n",
    "    ],\n",
    "    \"attn_5\": [\n",
    "    ],\n",
    "    \"mlp_5\": [\n",
    "    ],\n",
    "    \"resid_5\": [\n",
    "        2864, # \"her\"\n",
    "        11682, # \"he\"\n",
    "    ],\n",
    "    \"attn_6\": [\n",
    "    ],\n",
    "    \"mlp_6\": [\n",
    "    ],\n",
    "    \"resid_6\": [\n",
    "        4068, # female pronouns\n",
    "        7008, # gendered pronouns\n",
    "    ],\n",
    "    \"attn_7\": [\n",
    "    ],\n",
    "    \"mlp_7\": [\n",
    "    ],\n",
    "    \"resid_7\": [\n",
    "        7111, # text about women\n",
    "    ],\n",
    "    \"attn_8\": [\n",
    "    ],\n",
    "    \"mlp_8\": [\n",
    "    ],\n",
    "    \"resid_8\": [\n",
    "        6952, # gendered pronouns\n",
    "        9949, # female pronouns\n",
    "    ],\n",
    "    \"attn_9\": [\n",
    "    ],\n",
    "    \"mlp_9\": [\n",
    "    ],\n",
    "    \"resid_9\": [\n",
    "        6952, # \"she\"\n",
    "        15246, # female pronouns\n",
    "    ],\n",
    "    \"attn_10\": [\n",
    "    ],\n",
    "    \"mlp_10\": [\n",
    "    ],\n",
    "    \"resid_10\": [\n",
    "        3711, # promotes female-associated words\n",
    "        6952, # masculine pronouns\n",
    "    ],\n",
    "    \"attn_11\": [\n",
    "    ],\n",
    "    \"mlp_11\": [\n",
    "    ],\n",
    "    \"resid_11\": [\n",
    "        3013, # descriptions of women\n",
    "        4467, # \"He\"\n",
    "        11649, # \"she\", \"her\"\n",
    "    ],\n",
    "    \"attn_12\": [\n",
    "    ],\n",
    "    \"mlp_12\": [\n",
    "    ],\n",
    "    \"resid_12\": [\n",
    "        6335, # \"He\"\n",
    "        11114, # descriptions of women\n",
    "        11480, # \"his\", \"her\"\n",
    "    ],\n",
    "    \"attn_13\": [\n",
    "    ],\n",
    "    \"mlp_13\": [\n",
    "    ],\n",
    "    \"resid_13\": [\n",
    "        192, # descriptions of women\n",
    "        495, # \"her\"\n",
    "        14755, # \"he\"\n",
    "    ],\n",
    "    \"attn_14\": [\n",
    "    ],\n",
    "    \"mlp_14\": [\n",
    "    ],\n",
    "    \"resid_14\": [\n",
    "        2354, # descriptions of women\n",
    "        12665, # male pronouns\n",
    "    ],\n",
    "    \"attn_15\": [\n",
    "    ],\n",
    "    \"mlp_15\": [\n",
    "    ],\n",
    "    \"resid_15\": [\n",
    "        798, # promotes feminine pronouns\n",
    "        6211, # gendered pronouns\n",
    "    ],\n",
    "    \"attn_16\": [\n",
    "    ],\n",
    "    \"mlp_16\": [\n",
    "    ],\n",
    "    \"resid_16\": [\n",
    "        6047, # women, girls, women's names\n",
    "        15567, # \"he\", \"she\"\n",
    "        16351, # promotes feminine pronouns\n",
    "    ],\n",
    "    \"attn_17\": [\n",
    "    ],\n",
    "    \"mlp_17\": [\n",
    "    ],\n",
    "    \"resid_17\": [\n",
    "        6011, # promotes feminine pronouns\n",
    "    ],\n",
    "    \"attn_18\": [\n",
    "    ],\n",
    "    \"mlp_18\": [\n",
    "    ],\n",
    "    \"resid_18\": [\n",
    "        61, # descriptions of female royalty\n",
    "    ],\n",
    "    \"attn_19\": [\n",
    "    ],\n",
    "    \"mlp_19\": [\n",
    "    ],\n",
    "    \"resid_19\": [\n",
    "        13002, # clauses starting with \"she\"\n",
    "    ],\n",
    "    \"attn_20\": [\n",
    "        9711, # \"her\", \"she\"\n",
    "    ],\n",
    "    \"mlp_20\": [\n",
    "    ],\n",
    "    \"resid_20\": [\n",
    "        7116, # promotes masculine pronouns\n",
    "        # 7324, # nursing\n",
    "        12545, # promotes female pronouns\n",
    "    ],\n",
    "    \"attn_21\": [\n",
    "        2740, # promotes feminine pronouns\n",
    "        10118, # masculine pronouns\n",
    "    ],\n",
    "    \"mlp_21\": [\n",
    "    ],\n",
    "    \"resid_21\": [\n",
    "        1065, # promotes masculine pronouns\n",
    "        # 1653, # nursing\n",
    "        4430, # promotes feminine pronouns\n",
    "    ],\n",
    "    \"attn_22\": [\n",
    "    ],\n",
    "    \"mlp_22\": [\n",
    "    ],\n",
    "    \"resid_22\": [\n",
    "        1208, # promotes masculine pronouns\n",
    "        3497, # promotes feminine pronouns\n",
    "        # 7961, # nursing\n",
    "    ],\n",
    "}\n",
    "print(f\"number of features to ablate: {sum([len(v) for v in feats_to_ablate.values()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting feats_to_ablate in a more useful format\n",
    "def n_hot(feats, dim):\n",
    "    out = t.zeros(dim, dtype=t.bool, device=DEVICE)\n",
    "    for feat in feats:\n",
    "        out[feat] = True\n",
    "    return out\n",
    "\n",
    "feats_to_ablate = {\n",
    "    submodule : n_hot(feats_to_ablate[submodule.name], dictionaries[submodule].dict_size) for submodule in submodules\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def collect_acts_ablated(\n",
    "    text_batches,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate,\n",
    "    layer,\n",
    "):\n",
    "    with tqdm(total=len(text_batches), desc=\"Collecting activations with ablations\") as pbar:\n",
    "        for text, *labels in text_batches:\n",
    "            with model.trace(text, **tracer_kwargs):\n",
    "                for submodule in submodules:\n",
    "                    dictionary = dictionaries[submodule]\n",
    "                    feat_idxs = to_ablate[submodule]\n",
    "                    if len(feat_idxs) == 0:\n",
    "                        continue\n",
    "                    x = submodule.get_activation()\n",
    "                    x_hat, f = dictionary(x, output_features=True)\n",
    "                    res = x - x_hat\n",
    "                    f[:, :,feat_idxs] = 0. # zero ablation\n",
    "                    submodule.set_activation(dictionary.decode(f) + res)\n",
    "                attn_mask = model.input[1]['attention_mask']\n",
    "                act = model.model.layers[layer].output[0]\n",
    "                pooled_act = pool_acts(act, attn_mask).save()\n",
    "            yield pooled_act.value, *labels\n",
    "            pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy after ablating features judged irrelevant by human annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations with ablations: 100%|██████████| 269/269 [02:33<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.7659153938293457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations with ablations: 100%|██████████| 55/55 [00:31<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth accuracy: 0.7603686451911926\n",
      "Spurious accuracy: 0.514976978302002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ambiguous_accs = test_probe(\n",
    "    probe,\n",
    "    activation_batches=collect_acts_ablated(\n",
    "        get_text_batches(split=\"test\", ambiguous=True), \n",
    "        model, submodules, dictionaries, feats_to_ablate, layer\n",
    "    ),\n",
    ")\n",
    "print(f\"Ambiguous test accuracy: {ambiguous_accs[0]}\")\n",
    "\n",
    "unambiguous_accs = test_probe(\n",
    "    probe,\n",
    "    activation_batches=collect_acts_ablated(\n",
    "        get_text_batches(split=\"test\", ambiguous=False),\n",
    "        model, submodules, dictionaries, feats_to_ablate, layer\n",
    "    ),\n",
    ")\n",
    "print(f\"Ground truth accuracy: {unambiguous_accs[0]}\")\n",
    "print(f\"Spurious accuracy: {unambiguous_accs[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept bottleneck probing baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = [    \n",
    "    ' nurse',\n",
    "    ' healthcare',\n",
    "    ' hospital',\n",
    "    ' patient',\n",
    "    ' medical',\n",
    "    ' clinic',\n",
    "    ' triage',\n",
    "    ' medication',\n",
    "    ' emergency',\n",
    "    ' surgery',\n",
    "    ' professor',\n",
    "    ' academia',\n",
    "    ' research',\n",
    "    ' university',\n",
    "    ' tenure',\n",
    "    ' faculty',\n",
    "    ' dissertation',\n",
    "    ' sabbatical',\n",
    "    ' publication',\n",
    "    ' grant',\n",
    "]\n",
    "# get concept vectors\n",
    "with t.no_grad(), model.trace(concepts):\n",
    "    concept_vectors = model.model.layers[layer].output[0][:, -1, :].save()\n",
    "concept_vectors = concept_vectors.value - concept_vectors.value.mean(0, keepdim=True)\n",
    "\n",
    "def get_bottleneck(text):\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        acts = model.model.layers[layer].output[0]\n",
    "        acts = acts * attn_mask[:, :, None]\n",
    "        acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        # compute cosine similarity with concept vectors\n",
    "        sims = (acts @ concept_vectors.T) / (acts.norm(dim=-1)[:, None] @ concept_vectors.norm(dim=-1)[None])\n",
    "        sims = sims.save()\n",
    "    return sims.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 0:  [0.6971435546875, 0.7413330078125, 0.70391845703125, 0.6898193359375, 0.67218017578125, 0.67254638671875, 0.699127197265625, 0.68316650390625, 0.68304443359375, 0.6861419677734375, 0.6903018951416016, 0.6856677532196045, 0.6833877563476562, 0.67919921875, 0.6834793090820312, 0.6787109375, 0.6787109375, 0.6835174560546875, 0.673492431640625, 0.6913928985595703, 0.666854977607727, 0.6630859375, 0.676666259765625, 0.679840087890625, 0.6655349731445312, 0.6761436462402344, 0.6749162673950195, 0.6784286499023438, 0.6638031005859375, 0.6699142456054688, 0.6470947265625, 0.6814117431640625, 0.6573638916015625, 0.665270209312439, 0.636077880859375, 0.6762542724609375, 0.6952438354492188, 0.6825408935546875, 0.691162109375, 0.661895751953125, 0.63525390625, 0.685699462890625, 0.6450138092041016, 0.660736083984375, 0.66497802734375, 0.63134765625, 0.6502838134765625, 0.65863037109375, 0.6770496368408203, 0.6722412109375, 0.6806640625, 0.638916015625, 0.65374755859375, 0.6380615234375, 0.63134765625, 0.6540679931640625, 0.654796302318573, 0.6394729614257812, 0.638671875, 0.6425247192382812, 0.6920166015625, 0.6500701904296875, 0.65228271484375, 0.636322021484375, 0.6435546875, 0.630859375, 0.6668548583984375, 0.65228271484375, 0.670166015625, 0.6583185195922852, 0.6387481689453125, 0.630859375, 0.6336669921875, 0.63665771484375, 0.60498046875, 0.6533203125, 0.630859375, 0.6295166015625, 0.6122512817382812, 0.68463134765625, 0.624481201171875, 0.696044921875, 0.640167236328125, 0.608154296875, 0.675537109375, 0.63055419921875, 0.61767578125, 0.632568359375, 0.651123046875, 0.6365966796875, 0.6413116455078125, 0.6076812744140625, 0.6125068664550781, 0.603668212890625, 0.6214580535888672, 0.6419577598571777, 0.60125732421875, 0.637451171875, 0.62646484375, 0.6057891845703125, 0.595947265625, 0.589752197265625, 0.6173095703125, 0.55029296875, 0.621185302734375, 0.634246826171875, 0.61083984375, 0.61376953125, 0.6308555603027344, 0.6380615234375, 0.60009765625, 0.61956787109375, 0.59661865234375, 0.686309814453125, 0.620391845703125, 0.5811233520507812, 0.59375, 0.6350784301757812, 0.63897705078125, 0.60137939453125, 0.626617431640625, 0.660736083984375, 0.60986328125, 0.60528564453125, 0.58251953125, 0.601531982421875, 0.60888671875, 0.5772705078125, 0.5977783203125, 0.57958984375, 0.607666015625, 0.6390380859375, 0.6212158203125, 0.6283245086669922, 0.6025390625, 0.6025390625, 0.564208984375, 0.62127685546875, 0.60205078125, 0.59033203125, 0.6005859375, 0.604461669921875, 0.537353515625, 0.57177734375, 0.611328125, 0.6064682006835938, 0.6103515625, 0.634765625, 0.55908203125, 0.60455322265625, 0.59423828125, 0.578125, 0.6060791015625, 0.56591796875, 0.60980224609375, 0.54052734375, 0.545166015625, 0.546142578125, 0.544921875, 0.67022705078125, 0.61474609375, 0.54296875, 0.5782470703125, 0.582275390625, 0.596954345703125, 0.565765380859375, 0.574951171875, 0.56280517578125, 0.6295585632324219, 0.6016845703125, 0.543212890625, 0.6207275390625, 0.50634765625, 0.603271484375, 0.590240478515625, 0.5870704650878906, 0.58795166015625, 0.55426025390625, 0.59161376953125, 0.572021484375, 0.6170654296875, 0.53662109375, 0.59130859375, 0.619537353515625, 0.561004638671875, 0.5991702079772949, 0.5048828125, 0.56884765625, 0.5714111328125, 0.5841064453125, 0.577392578125, 0.515625, 0.588104248046875, 0.5613555908203125, 0.549560546875, 0.55865478515625, 0.540771484375, 0.5643310546875, 0.586395263671875, 0.589202880859375, 0.49267578125, 0.60467529296875, 0.603515625, 0.62939453125, 0.5667724609375, 0.5532379150390625, 0.494140625, 0.5484504699707031, 0.6279296875, 0.529052734375, 0.5916748046875, 0.56207275390625, 0.6359100341796875, 0.529052734375, 0.55224609375, 0.518798828125, 0.5823974609375, 0.541259765625, 0.590087890625, 0.5355224609375, 0.60400390625, 0.56793212890625, 0.5477294921875, 0.56964111328125, 0.521240234375, 0.6346435546875, 0.50390625, 0.488525390625, 0.52685546875, 0.518310546875, 0.505126953125, 0.493408203125, 0.49365234375, 0.5703125, 0.5133209228515625, 0.465087890625, 0.55810546875, 0.534912109375, 0.54840087890625, 0.6033935546875, 0.5197544097900391, 0.47607421875, 0.579833984375, 0.428466796875, 0.5167312622070312, 0.568572998046875, 0.578857421875, 0.496826171875, 0.5446014404296875, 0.535064697265625, 0.58935546875, 0.52587890625, 0.640869140625, 0.52728271484375, 0.664794921875, 0.452880859375, 0.46142578125, 0.5009765625, 0.49957275390625, 0.477294921875, 0.5625, 0.51824951171875, 0.6776123046875, 0.614013671875, 0.504150390625, 0.51171875, 0.53125, 0.540283203125, 0.573486328125, 0.501861572265625, 0.579345703125, 0.665771484375, 0.5279693603515625, 0.5457763671875, 0.46142578125, 0.4921875, 0.5356407165527344, 0.5166015625, 0.609375, 0.493408203125, 0.49639892578125, 0.454833984375, 0.568359375, 0.499267578125, 0.480712890625, 0.42724609375, 0.64080810546875, 0.513641357421875, 0.552734375, 0.478515625, 0.5631103515625, 0.522705078125, 0.5494384765625, 0.465576171875, 0.561279296875, 0.603759765625, 0.536865234375, 0.52880859375, 0.584228515625, 0.486328125, 0.5394287109375, 0.4814453125, 0.46533203125, 0.464599609375, 0.5728759765625, 0.50439453125, 0.462890625, 0.50091552734375, 0.4814453125, 0.519775390625, 0.44287109375, 0.435791015625, 0.5240478515625, 0.54766845703125, 0.45849609375, 0.4420166015625, 0.464080810546875, 0.46044921875, 0.465087890625, 0.457763671875, 0.4911041259765625, 0.524169921875, 0.515380859375, 0.548095703125, 0.48919677734375, 0.629058837890625, 0.5155029296875, 0.493408203125, 0.4912109375, 0.5252685546875, 0.4677734375, 0.5145263671875, 0.46630859375, 0.5, 0.541015625, 0.51336669921875, 0.57220458984375, 0.49395751953125, 0.490234375, 0.466796875, 0.510986328125, 0.51495361328125, 0.5892333984375, 0.57000732421875, 0.43017578125, 0.44580078125, 0.478271484375, 0.46044921875, 0.506103515625, 0.5057373046875, 0.5758056640625, 0.56982421875, 0.529296875, 0.421875, 0.513427734375, 0.5899658203125, 0.5361328125, 0.5615234375, 0.515869140625, 0.599365234375, 0.48248291015625, 0.5042877197265625, 0.454833984375, 0.5360107421875, 0.5376739501953125, 0.538818359375, 0.627197265625, 0.5029296875, 0.549560546875, 0.43310546875, 0.506591796875, 0.485107421875, 0.54052734375, 0.54150390625, 0.474365234375, 0.499267578125, 0.581329345703125, 0.7135009765625, 0.5389404296875, 0.534912109375, 0.4501953125, 0.549560546875, 0.5318756103515625, 0.45263671875, 0.51708984375, 0.4716796875, 0.476318359375, 0.571044921875, 0.478271484375, 0.5589599609375, 0.514892578125, 0.56591796875, 0.541748046875, 0.478271484375, 0.5319747924804688, 0.478759765625, 0.50030517578125, 0.55078125, 0.450927734375, 0.456298828125, 0.58642578125, 0.4915771484375, 0.52972412109375, 0.589599609375, 0.499755859375, 0.483642578125, 0.450927734375, 0.464599609375, 0.4375, 0.515869140625, 0.476806640625, 0.50390625, 0.54541015625, 0.5989990234375, 0.5517578125, 0.510986328125, 0.462158203125, 0.4794921875, 0.4814453125, 0.5888671875, 0.563232421875, 0.497802734375, 0.476806640625, 0.47283935546875, 0.51983642578125, 0.4921875, 0.5380859375, 0.5172119140625, 0.5537109375, 0.47216796875, 0.496826171875, 0.488525390625, 0.5467529296875, 0.5196533203125, 0.4951171875, 0.4658203125, 0.607177734375, 0.510986328125, 0.545654296875, 0.460205078125, 0.469482421875, 0.508056640625, 0.5825901031494141, 0.504638671875, 0.49365234375, 0.471923828125, 0.51904296875, 0.514404296875, 0.606201171875, 0.511474609375, 0.571319580078125, 0.5859375, 0.479248046875, 0.498779296875, 0.47802734375, 0.53662109375, 0.456512451171875, 0.50537109375, 0.48101043701171875, 0.571533203125, 0.471923828125, 0.52294921875, 0.61376953125, 0.4856414794921875, 0.446044921875, 0.47412109375, 0.562896728515625, 0.536865234375, 0.446533203125, 0.538482666015625, 0.551513671875, 0.457275390625, 0.419677734375, 0.5693359375, 0.514404296875, 0.52655029296875, 0.463623046875, 0.5779571533203125, 0.578369140625, 0.422119140625, 0.50701904296875, 0.471923828125, 0.483642578125, 0.48996734619140625, 0.497802734375, 0.5576171875, 0.530517578125, 0.486083984375, 0.54736328125, 0.541259765625, 0.5577392578125, 0.514923095703125, 0.517333984375, 0.478759765625, 0.54931640625, 0.4755859375, 0.51708984375, 0.590087890625, 0.435546875, 0.5355224609375, 0.4296875, 0.52264404296875, 0.462158203125, 0.49365234375, 0.46728515625, 0.61474609375, 0.49609375, 0.599609375, 0.508697509765625, 0.5521240234375, 0.452880859375, 0.500732421875, 0.491455078125, 0.484619140625, 0.467529296875, 0.50341796875, 0.470458984375, 0.502197265625, 0.467041015625, 0.497467041015625, 0.587493896484375, 0.509033203125, 0.472625732421875, 0.5628662109375, 0.52117919921875, 0.6043701171875, 0.470703125, 0.4892578125, 0.57037353515625, 0.45104217529296875, 0.47705078125, 0.44580078125, 0.540283203125, 0.4752197265625, 0.498046875, 0.43359375, 0.4765625, 0.5966796875, 0.4976806640625, 0.531982421875, 0.5685577392578125, 0.5506591796875, 0.468994140625, 0.4619140625, 0.46533203125, 0.5341796875, 0.5425262451171875, 0.470458984375, 0.48394775390625, 0.48138427734375, 0.48583984375, 0.4906005859375, 0.580322265625, 0.45263671875, 0.458740234375, 0.519775390625, 0.447265625, 0.51361083984375, 0.488037109375, 0.456298828125, 0.42236328125, 0.5423583984375, 0.435546875, 0.435546875, 0.48779296875, 0.52392578125, 0.542724609375, 0.462890625, 0.5244140625, 0.496337890625, 0.556243896484375, 0.5069646835327148, 0.5430498123168945, 0.498046875, 0.43212890625, 0.465606689453125, 0.5556640625, 0.431396484375, 0.503173828125, 0.4659423828125, 0.493896484375, 0.5338134765625, 0.5617828369140625, 0.560791015625, 0.426513671875, 0.514892578125, 0.432861328125, 0.53369140625, 0.5267333984375, 0.44287109375, 0.3857421875, 0.56060791015625, 0.492431640625, 0.6260986328125, 0.405029296875, 0.502685546875, 0.53466796875, 0.609619140625, 0.5025634765625, 0.462646484375, 0.648895263671875, 0.568115234375, 0.446533203125, 0.6275386810302734, 0.478515625, 0.471923828125, 0.47054147720336914, 0.453125, 0.507080078125, 0.477294921875, 0.40869140625, 0.526123046875, 0.573486328125, 0.50048828125, 0.4591064453125, 0.5247344970703125, 0.5048828125, 0.472900390625, 0.555908203125, 0.51953125, 0.5048828125, 0.481201171875, 0.5434740781784058, 0.41357421875, 0.4993896484375, 0.584014892578125, 0.476318359375, 0.5204238891601562, 0.4462890625, 0.44580078125, 0.60675048828125, 0.451416015625, 0.5709228515625, 0.481689453125, 0.435791015625, 0.480712890625, 0.5262451171875, 0.4873046875, 0.532470703125, 0.5557861328125, 0.582763671875, 0.517333984375, 0.42578125, 0.5068359375, 0.5732421875, 0.4345703125, 0.6131591796875, 0.425537109375, 0.419921875, 0.376953125, 0.397705078125, 0.502685546875, 0.627685546875, 0.56402587890625, 0.4560546875, 0.52783203125, 0.53564453125, 0.545654296875, 0.51739501953125, 0.427001953125, 0.6552658081054688, 0.5107421875, 0.525634765625, 0.47119140625, 0.47265625, 0.445068359375, 0.4970703125, 0.5166015625, 0.464599609375, 0.4820556640625, 0.474365234375, 0.477996826171875, 0.415283203125, 0.52099609375, 0.416015625, 0.6530609130859375, 0.503662109375, 0.50927734375, 0.5386962890625, 0.39794921875, 0.518310546875, 0.51953125, 0.45623779296875, 0.406982421875, 0.6493072509765625, 0.601806640625, 0.499267578125, 0.5198516845703125, 0.459228515625, 0.560546875, 0.51318359375, 0.52734375, 0.548828125, 0.508056640625, 0.437255859375, 0.60601806640625, 0.432861328125, 0.427001953125, 0.49072265625, 0.41162109375, 0.458740234375, 0.419921875, 0.505157470703125, 0.4326171875, 0.48944091796875, 0.43701171875, 0.47265625, 0.4970703125, 0.519775390625, 0.426025390625, 0.4710693359375, 0.4853515625, 0.458251953125, 0.40380859375, 0.46734619140625, 0.437255859375, 0.41650390625, 0.37158203125, 0.5069580078125, 0.43408203125, 0.4755859375, 0.4462890625, 0.4990234375, 0.640869140625, 0.4464111328125, 0.5029296875, 0.507080078125, 0.51617431640625, 0.47509765625, 0.580078125, 0.4931488037109375, 0.434814453125, 0.511474609375, 0.435546875, 0.398681640625, 0.5939235687255859, 0.35888671875, 0.44873046875, 0.42919921875, 0.50830078125, 0.44085693359375, 0.407470703125, 0.432373046875, 0.4700927734375, 0.41650390625, 0.46051025390625, 0.440185546875, 0.474609375, 0.394775390625, 0.4952392578125, 0.64111328125, 0.42724609375, 0.58782958984375, 0.52978515625, 0.41845703125, 0.45556640625, 0.6473388671875, 0.43994140625, 0.4102783203125, 0.415771484375, 0.45654296875, 0.56982421875, 0.408203125, 0.445556640625, 0.49273681640625, 0.562744140625, 0.48162078857421875, 0.4111328125, 0.4208984375, 0.6733245849609375, 0.593994140625, 0.4326171875, 0.4765625, 0.4140625, 0.46197509765625, 0.594970703125, 0.506591796875, 0.50347900390625, 0.486328125, 0.429931640625, 0.490966796875, 0.5367546081542969, 0.43017578125, 0.429443359375, 0.4676513671875, 0.4622802734375, 0.5244140625, 0.58251953125, 0.49853515625, 0.403564453125, 0.409423828125, 0.43017578125, 0.43701171875, 0.448944091796875, 0.506561279296875, 0.418212890625, 0.5168228149414062, 0.414794921875, 0.533447265625, 0.4501953125, 0.443603515625, 0.45068359375, 0.448974609375, 0.5278778076171875, 0.50146484375, 0.5816650390625, 0.43603515625, 0.51873779296875, 0.530029296875, 0.384521484375, 0.498779296875, 0.444091796875, 0.4723968505859375, 0.482421875, 0.4998779296875, 0.5202903747558594, 0.402099609375, 0.56982421875, 0.6466064453125, 0.425048828125, 0.374267578125, 0.523193359375, 0.4873046875, 0.437255859375, 0.360107421875, 0.44921875, 0.41845703125, 0.5400390625, 0.5333251953125, 0.519287109375, 0.4716796875, 0.60406494140625, 0.4736328125, 0.5367431640625, 0.547607421875, 0.431396484375, 0.453369140625, 0.481201171875, 0.43896484375, 0.4942626953125, 0.46307373046875, 0.556640625, 0.484619140625, 0.471038818359375, 0.4404296875, 0.46435546875, 0.485107421875, 0.438720703125, 0.48779296875, 0.4676513671875, 0.489013671875, 0.4361724853515625, 0.398193359375, 0.41357421875, 0.5816650390625, 0.489501953125, 0.4908447265625, 0.404296875, 0.44189453125, 0.456787109375, 0.4554443359375, 0.4710693359375, 0.440185546875, 0.427490234375, 0.568359375, 0.501708984375, 0.432373046875, 0.452392578125, 0.58740234375, 0.5836181640625, 0.494384765625, 0.509521484375, 0.47918701171875, 0.43505859375, 0.410400390625, 0.498779296875, 0.5107421875, 0.445068359375, 0.37841796875, 0.44580078125, 0.453857421875, 0.577239990234375, 0.45166015625, 0.468017578125, 0.4287109375, 0.4777374267578125, 0.453125, 0.488311767578125, 0.4560546875, 0.70166015625, 0.508056640625, 0.52301025390625, 0.46875, 0.44580078125, 0.5224609375, 0.3822021484375, 0.462890625, 0.4952392578125, 0.4375, 0.5869140625, 0.489501953125, 0.490234375, 0.512420654296875, 0.43603515625, 0.4537353515625, 0.495361328125, 0.5042724609375, 0.5838623046875, 0.49904632568359375, 0.490966796875, 0.4296875, 0.515625, 0.466796875, 0.57080078125, 0.4554443359375, 0.4753570556640625, 0.4873046875, 0.485626220703125, 0.4556884765625, 0.359130859375, 0.446044921875, 0.488616943359375, 0.5137939453125, 0.4556427001953125, 0.59033203125, 0.451904296875, 0.3818359375, 0.45947265625, 0.580810546875, 0.5118408203125, 0.35693359375, 0.447021484375, 0.484619140625, 0.5345458984375, 0.486572265625, 0.4461669921875, 0.432373046875, 0.5703125, 0.6220703125, 0.414306640625, 0.48779296875, 0.4853515625, 0.4775390625, 0.5257568359375, 0.5545654296875, 0.48095703125, 0.422607421875, 0.4365234375, 0.552001953125, 0.453857421875, 0.419189453125, 0.454345703125, 0.4900054931640625, 0.569580078125, 0.433837890625, 0.4189453125, 0.586181640625, 0.428955078125, 0.43505859375, 0.4384765625, 0.477508544921875, 0.46533203125, 0.42919921875, 0.45166015625, 0.742919921875, 0.44586181640625, 0.44482421875, 0.555419921875, 0.53350830078125, 0.430908203125, 0.409912109375, 0.54833984375, 0.426513671875, 0.54229736328125, 0.455078125, 0.51904296875, 0.4267578125, 0.572998046875, 0.4718017578125, 0.489013671875, 0.503173828125, 0.447021484375, 0.48486328125, 0.45623779296875, 0.4635009765625, 0.458984375, 0.445068359375, 0.425048828125, 0.536712646484375, 0.470947265625, 0.517578125, 0.38671875, 0.476318359375, 0.4091796875, 0.400146484375, 0.45501708984375, 0.434814453125, 0.402587890625, 0.4990234375, 0.39111328125, 0.55859375, 0.496337890625, 0.38525390625, 0.58251953125, 0.52215576171875, 0.41748046875, 0.448486328125, 0.372802734375, 0.520263671875, 0.48974609375, 0.4775390625, 0.446044921875, 0.4658203125, 0.4790191650390625, 0.3792724609375, 0.4306640625, 0.441650390625, 0.47442626953125, 0.53564453125, 0.5078125, 0.387939453125, 0.53466796875, 0.399169921875, 0.3876953125, 0.443634033203125, 0.447265625, 0.4248046875, 0.3912353515625, 0.54150390625, 0.3961639404296875, 0.447998046875, 0.44677734375, 0.47802734375, 0.374755859375, 0.43896484375, 0.5154364109039307, 0.6246337890625, 0.4678192138671875, 0.48388671875, 0.379150390625, 0.58282470703125, 0.493408203125, 0.414794921875, 0.4129638671875, 0.43212890625, 0.3406982421875, 0.506591796875, 0.5190153121948242, 0.4541015625, 0.462646484375, 0.5572509765625, 0.5570564270019531, 0.5433349609375, 0.437255859375, 0.55364990234375, 0.478759765625, 0.45703125, 0.4599609375, 0.3966064453125, 0.432861328125, 0.446044921875, 0.5723724365234375, 0.59271240234375, 0.4869384765625, 0.4637451171875, 0.453125, 0.44921875, 0.482666015625, 0.53399658203125, 0.509033203125, 0.55914306640625, 0.556884765625, 0.5333251953125, 0.46142578125, 0.4991455078125, 0.3934326171875, 0.491455078125, 0.44921875, 0.5098876953125, 0.466796875, 0.54132080078125, 0.42724609375, 0.458740234375, 0.4990234375, 0.4327392578125, 0.5404052734375, 0.498138427734375, 0.4359130859375, 0.424560546875, 0.462799072265625, 0.5325927734375, 0.475341796875, 0.5107421875, 0.61572265625, 0.42431640625, 0.50146484375, 0.39990234375, 0.51806640625, 0.4222412109375, 0.49853515625, 0.4931640625, 0.4150390625, 0.3896484375, 0.427978515625, 0.4678955078125, 0.517822265625, 0.3687744140625, 0.391845703125, 0.546630859375, 0.417572021484375, 0.52899169921875, 0.56689453125, 0.406005859375, 0.516510009765625, 0.493408203125, 0.52197265625, 0.49462890625, 0.4500274658203125, 0.5257568359375, 0.42431640625, 0.3746337890625, 0.5330810546875, 0.412109375, 0.39892578125, 0.429931640625, 0.429931640625, 0.5192718505859375, 0.44970703125, 0.389892578125, 0.4425048828125, 0.450439453125, 0.53656005859375, 0.36865234375, 0.396484375, 0.48486328125, 0.445556640625, 0.592926025390625, 0.39990234375, 0.372314453125, 0.5865478515625, 0.481689453125, 0.47341156005859375, 0.4802360534667969, 0.43017578125, 0.4033203125, 0.472900390625, 0.387939453125, 0.45989990234375, 0.48974609375, 0.4322509765625, 0.671142578125, 0.462890625, 0.450653076171875, 0.409912109375, 0.4488525390625, 0.4227294921875, 0.5091552734375, 0.538818359375, 0.434814453125, 0.419921875, 0.431396484375, 0.474609375, 0.4267578125, 0.4951171875, 0.49420166015625, 0.493896484375, 0.535797119140625, 0.536590576171875, 0.4781494140625, 0.48486328125, 0.3431396484375, 0.48211669921875, 0.406005859375, 0.50341796875, 0.450408935546875, 0.4535980224609375, 0.54345703125, 0.461181640625, 0.420654296875, 0.49314117431640625, 0.431396484375, 0.35693359375, 0.4442596435546875, 0.43359375, 0.514404296875, 0.370849609375, 0.39599609375, 0.463531494140625, 0.508056640625, 0.435302734375, 0.5121612548828125, 0.478607177734375, 0.54541015625, 0.458984375, 0.433349609375, 0.4833984375, 0.48581790924072266, 0.478759765625, 0.4921875, 0.510498046875, 0.46533203125, 0.39794921875, 0.429931640625, 0.480712890625, 0.45947265625, 0.3856201171875, 0.471923828125, 0.45953369140625, 0.6038818359375, 0.4191017150878906, 0.398681640625, 0.54541015625, 0.3897705078125, 0.352783203125, 0.403076171875, 0.380615234375, 0.470794677734375, 0.362060546875, 0.4007568359375, 0.2987060546875, 0.3623046875, 0.6600341796875, 0.539886474609375, 0.386474609375, 0.40283203125, 0.5941162109375, 0.4306640625, 0.509521484375, 0.403564453125, 0.42919921875, 0.406982421875, 0.41943359375, 0.5458984375, 0.458251953125, 0.510772705078125, 0.4306640625, 0.3857421875, 0.5179443359375, 0.4559326171875, 0.4443359375, 0.4515380859375, 0.403076171875, 0.456634521484375, 0.462158203125, 0.45458984375, 0.364990234375, 0.3980712890625, 0.5472412109375, 0.386474609375, 0.64617919921875, 0.516845703125, 0.45343017578125, 0.37939453125, 0.568603515625, 0.37646484375, 0.408935546875, 0.4525146484375, 0.419921875, 0.388427734375, 0.4330902099609375, 0.45465707778930664, 0.52099609375, 0.51416015625, 0.402587890625, 0.391357421875, 0.4041748046875, 0.38720703125, 0.426025390625, 0.39453125, 0.39990234375, 0.43701171875, 0.411376953125, 0.41162109375, 0.423095703125, 0.49267578125, 0.5364990234375, 0.49755859375, 0.41400146484375, 0.5302734375, 0.4644775390625, 0.41455078125, 0.414306640625, 0.498046875, 0.3931884765625, 0.46783447265625, 0.531463623046875, 0.53955078125, 0.415771484375, 0.499755859375, 0.491943359375, 0.5927734375, 0.372314453125, 0.395263671875, 0.5028076171875, 0.497283935546875, 0.4739990234375, 0.37738037109375, 0.466064453125, 0.429931640625, 0.4365234375, 0.419921875, 0.371337890625, 0.42236328125, 0.387939453125, 0.47235107421875, 0.3741455078125, 0.672119140625, 0.361572265625, 0.4091796875, 0.5115966796875, 0.507720947265625, 0.494384765625, 0.4593505859375, 0.429931640625, 0.4091796875, 0.4627685546875, 0.520751953125, 0.426513671875, 0.4865264892578125, 0.58819580078125, 0.4297599792480469, 0.394775390625, 0.482269287109375, 0.473876953125, 0.4979248046875, 0.4134521484375, 0.537841796875, 0.400146484375, 0.4576873779296875, 0.4661865234375, 0.39404296875, 0.48193359375, 0.45751953125, 0.45361328125, 0.4462890625, 0.4302978515625, 0.5341796875, 0.447021484375, 0.4244384765625, 0.53271484375, 0.4972076416015625, 0.48583984375, 0.5660400390625, 0.39013671875, 0.5306396484375, 0.576171875, 0.455810546875, 0.4857025146484375, 0.441162109375, 0.408203125, 0.41650390625, 0.453125, 0.39990234375, 0.5205078125, 0.4639892578125, 0.5974044799804688, 0.4652099609375, 0.5721435546875, 0.539306640625, 0.7332305908203125, 0.48305511474609375, 0.4134521484375, 0.419921875, 0.52099609375, 0.423583984375, 0.4061279296875, 0.3712158203125, 0.4991455078125, 0.4150390625, 0.45263671875, 0.4830322265625, 0.5363540649414062, 0.389892578125, 0.45361328125, 0.5240020751953125, 0.41656494140625, 0.5079345703125, 0.5218505859375, 0.4404296875, 0.5498046875, 0.4136505126953125, 0.3812255859375, 0.4207763671875, 0.387451171875, 0.5098876953125, 0.4444580078125, 0.618682861328125, 0.517578125, 0.48443603515625, 0.6519775390625, 0.3529052734375, 0.411376953125, 0.539794921875, 0.453094482421875, 0.415771484375, 0.415771484375, 0.3946533203125, 0.4351806640625, 0.49993896484375, 0.463623046875, 0.661590576171875, 0.4173583984375, 0.405364990234375, 0.4844970703125, 0.40380859375, 0.47607421875, 0.3607177734375, 0.41204833984375, 0.45953369140625, 0.509033203125, 0.440673828125, 0.399658203125, 0.498779296875, 0.48779296875, 0.443115234375, 0.5233154296875, 0.43548583984375, 0.388671875, 0.30615234375, 0.428466796875, 0.587646484375, 0.37158203125, 0.466064453125, 0.373779296875, 0.635009765625, 0.5543212890625, 0.43994140625, 0.429443359375, 0.415283203125, 0.3985137939453125, 0.3756103515625, 0.39107322692871094, 0.4306640625, 0.494140625, 0.414794921875, 0.4764404296875, 0.39306640625, 0.4288330078125, 0.489013671875, 0.541748046875, 0.5196533203125, 0.38916015625, 0.4913330078125, 0.4415779113769531, 0.465087890625, 0.515869140625, 0.5120849609375, 0.44677734375, 0.3153076171875, 0.4791259765625, 0.421875, 0.533203125, 0.4979248046875, 0.367919921875, 0.4193115234375, 0.5263671875, 0.417236328125, 0.38592529296875, 0.539306640625, 0.447021484375, 0.50701904296875, 0.47786712646484375, 0.3818359375, 0.373291015625, 0.40283203125, 0.424072265625, 0.456787109375, 0.421142578125, 0.4833984375, 0.4706573486328125, 0.408203125, 0.379150390625, 0.516845703125, 0.442138671875, 0.434814453125, 0.5687255859375, 0.40234375, 0.40283203125, 0.5203857421875, 0.52587890625, 0.4876708984375, 0.43017578125, 0.3348388671875, 0.414306640625, 0.46875, 0.401611328125, 0.3604736328125, 0.4423828125, 0.53173828125, 0.523681640625, 0.6072998046875, 0.4405517578125, 0.5003662109375, 0.3681640625, 0.471435546875, 0.47705078125, 0.539306640625, 0.44921875, 0.475341796875, 0.462493896484375, 0.5172882080078125, 0.42529296875, 0.391357421875, 0.4814453125, 0.546142578125, 0.400634765625, 0.42041015625, 0.503173828125, 0.421875, 0.3779296875, 0.4102783203125, 0.5593490600585938, 0.3934326171875, 0.5450439453125, 0.39599609375, 0.4654541015625, 0.37744140625, 0.462646484375, 0.44586181640625, 0.4693603515625, 0.360107421875, 0.478515625, 0.42138671875, 0.443359375, 0.428466796875, 0.587371826171875, 0.47922515869140625, 0.48974609375, 0.637939453125, 0.43359375, 0.406494140625, 0.42578125, 0.45819854736328125, 0.427001953125, 0.38153076171875, 0.3857421875, 0.4683837890625, 0.407958984375, 0.4835205078125, 0.593048095703125, 0.5078125, 0.7220458984375, 0.429931640625, 0.357421875, 0.49755859375, 0.42529296875, 0.37255859375, 0.4248046875, 0.52899169921875, 0.387939453125, 0.3828125, 0.3975830078125, 0.413330078125, 0.410888671875, 0.509033203125, 0.42041015625, 0.44537353515625, 0.399169921875, 0.5052490234375, 0.40673828125, 0.40673828125, 0.4019775390625, 0.419921875, 0.52978515625, 0.341796875, 0.441650390625, 0.5123291015625, 0.371337890625, 0.394287109375, 0.4459228515625, 0.39453125, 0.528564453125, 0.504974365234375, 0.67877197265625, 0.430419921875, 0.36474609375, 0.380615234375, 0.408935546875, 0.49609375, 0.5913028717041016, 0.37841796875, 0.3536376953125, 0.56103515625, 0.3394775390625, 0.485595703125, 0.4315185546875, 0.492431640625, 0.392333984375, 0.485076904296875, 0.514404296875, 0.469390869140625, 0.599853515625, 0.4918212890625, 0.4794921875, 0.31103515625, 0.4346923828125, 0.4609375, 0.448486328125, 0.45931243896484375, 0.35888671875, 0.3399658203125, 0.460693359375, 0.4207763671875, 0.47216796875, 0.4747314453125, 0.4239501953125, 0.3697509765625, 0.484619140625, 0.549072265625, 0.4683837890625, 0.48681640625, 0.47900390625, 0.4239501953125, 0.3502197265625, 0.529541015625, 0.4937744140625, 0.452392578125, 0.43603515625, 0.398193359375, 0.3775634765625, 0.72802734375, 0.576416015625, 0.351318359375, 0.4595069885253906, 0.5404739379882812, 0.56396484375, 0.4169921875, 0.47705078125, 0.45172119140625, 0.41796875, 0.4208984375, 0.593505859375, 0.535400390625, 0.5618896484375, 0.3629150390625, 0.393798828125, 0.39404296875, 0.4371337890625, 0.43896484375, 0.4987945556640625, 0.38818359375, 0.3367919921875, 0.391845703125, 0.5040283203125, 0.48046875, 0.43017578125, 0.406005859375, 0.555908203125, 0.43408203125, 0.3782958984375, 0.43408203125, 0.50341796875, 0.414306640625, 0.431640625, 0.422607421875, 0.4140625, 0.3433837890625, 0.3582763671875, 0.36474609375, 0.551300048828125, 0.4072265625, 0.389404296875, 0.502197265625, 0.439208984375, 0.38232421875, 0.45892333984375, 0.436279296875, 0.437255859375, 0.5645751953125, 0.5365447998046875, 0.477294921875, 0.49176025390625, 0.38525390625, 0.468719482421875, 0.345703125, 0.412109375, 0.5328369140625, 0.4814453125, 0.660888671875, 0.6151123046875, 0.426513671875, 0.42431640625, 0.43413543701171875, 0.3436279296875, 0.3870849609375, 0.40576171875, 0.492919921875, 0.378173828125, 0.4365234375, 0.57025146484375, 0.42974853515625, 0.384033203125, 0.37939453125, 0.5304412841796875, 0.43017578125, 0.49267578125, 0.3450927734375, 0.367919921875, 0.453857421875, 0.4522705078125, 0.536865234375, 0.4150390625, 0.5040283203125, 0.3912353515625, 0.52978515625, 0.4951171875, 0.384765625, 0.46148681640625, 0.42724609375, 0.397216796875, 0.364501953125, 0.408477783203125, 0.5819091796875, 0.451904296875, 0.408203125, 0.441162109375, 0.470947265625, 0.3946533203125, 0.35009765625, 0.42181396484375, 0.38330078125, 0.50030517578125, 0.49554443359375, 0.45843505859375, 0.350830078125, 0.48095703125, 0.5166015625, 0.4794921875, 0.402099609375, 0.39306640625, 0.376708984375, 0.408447265625, 0.4072265625, 0.4476318359375, 0.46923828125, 0.4501953125, 0.442626953125, 0.459228515625, 0.404541015625, 0.3609619140625, 0.464111328125, 0.496826171875, 0.4150390625, 0.5076904296875, 0.44219970703125, 0.3896484375, 0.4854736328125, 0.47021484375, 0.3260498046875, 0.41510009765625, 0.437164306640625, 0.40887451171875, 0.570556640625, 0.3720703125, 0.419921875, 0.440673828125, 0.38525390625, 0.70941162109375, 0.430908203125, 0.451812744140625, 0.38037109375, 0.3897705078125, 0.3514404296875, 0.435302734375, 0.46826171875, 0.397216796875, 0.43359375, 0.558349609375, 0.494873046875, 0.5422096252441406, 0.3704833984375, 0.37841796875, 0.477783203125, 0.36669921875, 0.404052734375, 0.4508056640625, 0.506103515625, 0.378173828125, 0.484619140625, 0.584716796875, 0.537384033203125, 0.3343505859375, 0.46435546875, 0.484130859375, 0.409576416015625, 0.5426025390625, 0.5006103515625, 0.364013671875, 0.487060546875, 0.472259521484375, 0.49169921875, 0.427734375, 0.328125, 0.5914306640625, 0.481689453125, 0.341064453125, 0.5111541748046875, 0.36376953125, 0.3521728515625, 0.3966064453125, 0.4976806640625, 0.3642578125, 0.490966796875, 0.373291015625, 0.4033203125, 0.367401123046875, 0.53619384765625, 0.4970703125, 0.32275390625, 0.419921875, 0.384765625, 0.4801025390625, 0.5269775390625, 0.401123046875, 0.407470703125, 0.4908447265625, 0.42431640625, 0.408447265625, 0.38818359375, 0.391448974609375, 0.4287109375, 0.3848876953125, 0.47802734375, 0.37158203125, 0.445556640625, 0.4752655029296875, 0.43145751953125, 0.38720703125, 0.504638671875, 0.40966796875, 0.423828125, 0.4381103515625, 0.393310546875, 0.49648189544677734, 0.411865234375, 0.318603515625, 0.5303955078125, 0.4146728515625, 0.423583984375, 0.543212890625, 0.353759765625, 0.379150390625, 0.393798828125, 0.486083984375, 0.415283203125, 0.430419921875, 0.308837890625, 0.4583740234375, 0.4287109375, 0.37890625, 0.4368896484375, 0.354248046875, 0.46337890625, 0.3033447265625, 0.495849609375, 0.444091796875, 0.4185791015625, 0.397705078125, 0.4940185546875, 0.37939453125, 0.602294921875, 0.5198974609375, 0.43408203125, 0.45166015625, 0.369384765625, 0.454833984375, 0.4144287109375, 0.4390869140625, 0.4576416015625, 0.51361083984375, 0.450439453125, 0.385498046875, 0.580322265625, 0.464599609375, 0.44976806640625, 0.3538818359375, 0.444091796875, 0.47900390625, 0.4716796875, 0.61328125, 0.359375, 0.45263671875, 0.47149658203125, 0.412353515625, 0.46466064453125, 0.373291015625, 0.43310546875, 0.510986328125, 0.4865264892578125, 0.545654296875, 0.546875, 0.46826171875, 0.43408203125, 0.46533203125, 0.317138671875, 0.40625, 0.61181640625, 0.5732421875, 0.5390625, 0.394287109375, 0.439453125, 0.441650390625, 0.525390625, 0.437255859375, 0.455810546875, 0.4300537109375, 0.3814697265625, 0.57073974609375, 0.4189453125, 0.356201171875, 0.455810546875, 0.3692626953125, 0.4716796875, 0.46044921875, 0.4405517578125, 0.522216796875, 0.385986328125, 0.449951171875, 0.480224609375, 0.3907470703125, 0.522216796875, 0.333251953125, 0.44989013671875, 0.432373046875, 0.40185546875, 0.421142578125, 0.4375, 0.555419921875, 0.398681640625, 0.4805908203125, 0.404541015625, 0.539794921875, 0.434814453125, 0.423095703125, 0.392333984375, 0.501800537109375, 0.4111328125, 0.4207763671875, 0.4925537109375, 0.62066650390625, 0.411376953125, 0.4623985290527344, 0.3896484375, 0.342529296875, 0.44482421875, 0.4317626953125, 0.3948974609375, 0.4627685546875, 0.34814453125, 0.496826171875, 0.4058837890625, 0.3448486328125, 0.390380859375, 0.46533203125, 0.529693603515625, 0.438232421875, 0.61993408203125, 0.4639892578125, 0.4202880859375, 0.372802734375, 0.458984375, 0.337646484375, 0.4293212890625, 0.5069580078125, 0.40283203125, 0.4755859375, 0.44525146484375, 0.392333984375, 0.458984375, 0.3883056640625, 0.4378662109375, 0.368896484375, 0.3643798828125, 0.45702362060546875, 0.37603759765625, 0.442138671875, 0.45556640625, 0.5213623046875, 0.63134765625, 0.51904296875, 0.3636474609375, 0.3768310546875, 0.3770751953125, 0.3974609375, 0.3134765625, 0.4473876953125, 0.411376953125, 0.43115234375, 0.415771484375, 0.650390625, 0.4727783203125, 0.422119140625, 0.4395751953125, 0.522491455078125, 0.43896484375, 0.3729248046875, 0.562744140625, 0.4324951171875, 0.42079925537109375, 0.3785400390625, 0.370849609375, 0.3685302734375, 0.3388671875, 0.4853515625, 0.43798828125, 0.3292236328125, 0.469024658203125, 0.3707275390625, 0.47119140625, 0.52685546875, 0.40765380859375, 0.3553466796875, 0.410888671875, 0.5338134765625, 0.464111328125, 0.370849609375, 0.45458984375, 0.4825439453125, 0.3587646484375, 0.469970703125, 0.399658203125, 0.49468994140625, 0.308349609375, 0.408935546875, 0.551025390625, 0.5393218994140625, 0.548583984375, 0.500396728515625, 0.44635009765625, 0.392333984375, 0.4417724609375, 0.392333984375, 0.44287109375, 0.458740234375, 0.41015625, 0.3330078125, 0.438720703125, 0.527099609375, 0.363525390625, 0.291259765625, 0.4373779296875, 0.474853515625, 0.447265625, 0.4058837890625, 0.44287109375, 0.427490234375, 0.3837890625, 0.422607421875, 0.458984375, 0.451416015625, 0.36474609375, 0.4302520751953125, 0.3583984375, 0.3475341796875, 0.3345947265625, 0.4371337890625, 0.3394775390625, 0.3975830078125, 0.42333984375, 0.511962890625, 0.36083984375, 0.461669921875, 0.3558349609375, 0.584716796875, 0.398193359375, 0.43194580078125, 0.4130859375, 0.3919677734375, 0.541778564453125, 0.432373046875, 0.5103759765625, 0.3240966796875, 0.38037109375, 0.354248046875, 0.37060546875, 0.5426025390625, 0.4669189453125, 0.538818359375, 0.4266357421875, 0.3824462890625, 0.33740234375, 0.56591796875, 0.447509765625, 0.4525146484375, 0.47802734375, 0.4852294921875, 0.4151611328125, 0.392547607421875, 0.3583984375, 0.269775390625, 0.4892578125, 0.485595703125, 0.343994140625, 0.329345703125, 0.47332763671875, 0.4085693359375, 0.468994140625, 0.4141845703125, 0.447021484375, 0.350341796875, 0.42376708984375, 0.4345703125, 0.3177490234375, 0.35791015625, 0.39697265625, 0.4254150390625, 0.294677734375, 0.4217529296875, 0.410400390625, 0.431396484375, 0.379150390625, 0.456787109375, 0.410430908203125, 0.49456787109375, 0.43994140625, 0.434478759765625, 0.410888671875, 0.363037109375, 0.383056640625, 0.4478759765625, 0.47899627685546875, 0.46856689453125, 0.4971923828125, 0.50079345703125, 0.44036865234375, 0.441650390625, 0.396728515625, 0.434814453125, 0.4306640625, 0.4158935546875, 0.463623046875, 0.3865966796875, 0.427490234375, 0.39111328125, 0.2940673828125, 0.5019683837890625, 0.41357421875, 0.47589111328125, 0.45166015625, 0.3076171875, 0.446533203125, 0.40435791015625, 0.4345703125, 0.369873046875, 0.458251953125, 0.38604736328125, 0.56494140625, 0.46337890625, 0.44140625, 0.384033203125, 0.50439453125, 0.338134765625, 0.53973388671875, 0.43377685546875, 0.4197998046875, 0.41480255126953125, 0.4241943359375, 0.3441162109375, 0.5770263671875, 0.532958984375, 0.42034912109375, 0.3505859375, 0.44482421875, 0.53173828125, 0.3662109375, 0.5447998046875, 0.3515625, 0.4906005859375, 0.3758544921875, 0.448883056640625, 0.438720703125, 0.448486328125, 0.38232421875, 0.64825439453125, 0.3505859375, 0.565673828125, 0.4208984375, 0.316650390625, 0.4158935546875, 0.5458984375, 0.5152587890625, 0.35009765625, 0.39794921875, 0.3221435546875, 0.457275390625, 0.5369873046875, 0.40997314453125, 0.449951171875, 0.45947265625, 0.3700714111328125, 0.577392578125, 0.5159912109375, 0.3236083984375, 0.457763671875, 0.44189453125, 0.467529296875, 0.4185791015625, 0.50677490234375, 0.450439453125, 0.4005126953125, 0.4072265625, 0.38232421875, 0.421112060546875, 0.46923828125, 0.3946533203125, 0.42832183837890625, 0.468994140625, 0.5052490234375, 0.4033203125, 0.4500732421875, 0.423309326171875, 0.4921875, 0.3243408203125, 0.3690185546875, 0.36328125, 0.519775390625, 0.4439697265625, 0.51690673828125, 0.3271484375, 0.43701171875, 0.520263671875, 0.52001953125, 0.3460693359375, 0.43829345703125, 0.3482666015625, 0.5442657470703125, 0.546875, 0.380126953125, 0.542724609375, 0.48828125, 0.4398193359375, 0.441680908203125, 0.39208984375, 0.53955078125, 0.3585205078125, 0.3056640625, 0.3349609375, 0.335693359375, 0.524169921875, 0.461181640625, 0.434326171875, 0.3731689453125, 0.41064453125, 0.484619140625, 0.396484375, 0.389892578125, 0.515380859375, 0.455322265625, 0.36083984375, 0.571533203125, 0.50823974609375, 0.4580078125, 0.47509765625, 0.3687744140625, 0.4320068359375, 0.4915771484375, 0.448486328125, 0.4033203125, 0.54931640625, 0.4002685546875, 0.522216796875, 0.44873046875, 0.4755859375, 0.399658203125, 0.4510498046875, 0.3875732421875, 0.43896484375, 0.32568359375, 0.5001220703125, 0.43603515625, 0.4244384765625, 0.34740447998046875, 0.5325927734375, 0.44866943359375, 0.3499755859375, 0.353515625, 0.366943359375, 0.342041015625, 0.40484619140625, 0.46917724609375, 0.411865234375, 0.4161376953125, 0.5230712890625, 0.454345703125, 0.3326416015625, 0.3988037109375, 0.44569969177246094, 0.363037109375, 0.4576416015625, 0.2982177734375, 0.5654296875, 0.46044921875, 0.556884765625, 0.3172607421875, 0.398681640625, 0.4766845703125, 0.47589111328125, 0.433837890625, 0.5347900390625, 0.4407958984375, 0.4039306640625, 0.479736328125, 0.3642578125, 0.443115234375, 0.5247802734375, 0.54248046875, 0.337646484375, 0.5355224609375, 0.488525390625, 0.479736328125, 0.4050750732421875, 0.4302978515625, 0.5181884765625, 0.487640380859375, 0.575927734375, 0.3671875, 0.70556640625, 0.38330078125, 0.421875, 0.5482177734375, 0.39111328125, 0.3067626953125, 0.43310546875, 0.4599609375, 0.387939453125, 0.345947265625, 0.45654296875, 0.4747314453125, 0.4471435546875, 0.51806640625, 0.318115234375, 0.3753509521484375, 0.397705078125, 0.326416015625, 0.31524658203125, 0.4923095703125, 0.44863128662109375, 0.453369140625, 0.4566650390625, 0.495361328125, 0.361572265625, 0.45343017578125, 0.404510498046875, 0.51812744140625, 0.395751953125, 0.4390869140625, 0.3505859375, 0.55615234375, 0.509521484375, 0.61663818359375, 0.4811897277832031, 0.4517822265625, 0.4033203125, 0.2996826171875, 0.464111328125, 0.4146728515625, 0.4349365234375, 0.61700439453125, 0.471435546875, 0.36279296875, 0.39453125, 0.417724609375, 0.580078125, 0.4517822265625, 0.5091552734375, 0.34716796875, 0.399169921875, 0.4971923828125, 0.4488525390625, 0.372314453125, 0.344482421875, 0.544189453125, 0.4378662109375, 0.415771484375, 0.451171875, 0.41748046875, 0.572265625, 0.4215087890625, 0.3623046875, 0.4420166015625, 0.396728515625, 0.47509765625, 0.35595703125, 0.3209228515625, 0.3309326171875, 0.34619140625, 0.338623046875, 0.452392578125, 0.544677734375, 0.463134765625, 0.44384765625, 0.53472900390625, 0.571533203125, 0.558837890625, 0.32958984375, 0.521728515625, 0.51513671875, 0.499603271484375, 0.42138671875, 0.396728515625, 0.345947265625, 0.59228515625, 0.492919921875, 0.383056640625, 0.509033203125, 0.359130859375, 0.66986083984375, 0.32177734375, 0.470947265625, 0.539794921875, 0.375244140625, 0.399139404296875, 0.52001953125, 0.437255859375, 0.37255859375, 0.3887939453125, 0.405517578125, 0.385986328125, 0.365478515625, 0.346435546875, 0.4754638671875, 0.499755859375, 0.51025390625, 0.4774169921875, 0.576904296875, 0.480224609375, 0.396484375, 0.44189453125, 0.470703125, 0.3880157470703125, 0.40576171875, 0.372161865234375, 0.520263671875, 0.5616455078125, 0.3880615234375, 0.5086669921875, 0.3604736328125, 0.37451171875, 0.4366893768310547, 0.3414306640625, 0.4093017578125, 0.5555419921875, 0.550537109375, 0.4974365234375, 0.3927001953125, 0.44580078125, 0.48786163330078125, 0.525634765625, 0.4677734375, 0.3228759765625, 0.426513671875, 0.5656967163085938, 0.356689453125, 0.5045166015625, 0.451171875, 0.5302734375, 0.4005126953125, 0.415283203125, 0.361328125, 0.5275115966796875, 0.4559326171875, 0.53076171875, 0.518890380859375, 0.338623046875, 0.493896484375, 0.46978759765625, 0.35205078125, 0.4052734375, 0.403564453125, 0.31103515625, 0.45025634765625, 0.402099609375, 0.479248046875, 0.425537109375, 0.41943359375, 0.3389892578125, 0.381103515625, 0.33251953125, 0.35009765625, 0.481689453125, 0.434326171875, 0.33740234375, 0.3834228515625, 0.469024658203125, 0.476318359375, 0.33056640625, 0.46826171875, 0.36865234375, 0.355224609375, 0.41748046875, 0.3443603515625, 0.3623046875, 0.481689453125, 0.591552734375, 0.43017578125, 0.4208984375, 0.467041015625, 0.366455078125, 0.5986328125, 0.35888671875, 0.4530029296875, 0.65533447265625, 0.36767578125, 0.44598388671875, 0.3438720703125, 0.3126220703125, 0.449951171875, 0.395263671875, 0.386474609375, 0.418701171875, 0.3206787109375, 0.3785400390625, 0.4443359375, 0.32208251953125, 0.4635009765625, 0.329345703125, 0.421630859375, 0.3787841796875, 0.46051025390625, 0.4638671875, 0.49365234375, 0.496917724609375, 0.3563232421875, 0.5010223388671875, 0.3486328125, 0.4873046875, 0.4796295166015625, 0.337646484375, 0.371826171875, 0.4134521484375, 0.3070068359375, 0.4454345703125, 0.33935546875, 0.37000274658203125, 0.508544921875, 0.5184326171875, 0.455322265625, 0.36138916015625, 0.486083984375, 0.479736328125, 0.3831787109375, 0.435791015625, 0.40869140625, 0.429931640625, 0.4111328125, 0.3768310546875, 0.425537109375, 0.36474609375, 0.40765380859375, 0.409423828125, 0.4761962890625, 0.3369140625, 0.318603515625, 0.3984375, 0.454376220703125, 0.373291015625, 0.34912109375, 0.470458984375, 0.41546630859375, 0.46728515625, 0.3709716796875, 0.62261962890625, 0.4462890625, 0.3406982421875, 0.3857421875, 0.477630615234375, 0.457275390625, 0.425048828125, 0.432373046875, 0.373046875, 0.392333984375, 0.4312744140625, 0.5645751953125, 0.30712890625, 0.372314453125, 0.3331298828125, 0.31689453125, 0.388458251953125, 0.374267578125, 0.42626953125, 0.47900390625, 0.42767333984375, 0.48046875, 0.356689453125, 0.4930419921875, 0.48425769805908203, 0.4317626953125, 0.4141845703125, 0.429443359375, 0.46923828125, 0.3662109375, 0.427490234375, 0.416748046875, 0.5252685546875, 0.40185546875, 0.3541259765625, 0.6152420043945312, 0.583251953125, 0.448486328125, 0.4444580078125, 0.2786865234375, 0.3701171875, 0.5059814453125, 0.40234375, 0.48199462890625, 0.3974609375, 0.4688720703125, 0.3670654296875, 0.36328125, 0.462158203125, 0.4926300048828125, 0.4268798828125, 0.5771484375, 0.5185546875, 0.35205078125, 0.53759765625, 0.3681640625, 0.4439697265625, 0.4361572265625, 0.3729248046875, 0.4073486328125, 0.3482666015625, 0.650421142578125, 0.46044921875, 0.38861083984375, 0.406494140625, 0.3753662109375, 0.51666259765625, 0.4056396484375, 0.343994140625, 0.342529296875, 0.3494873046875, 0.43310546875, 0.459228515625, 0.416015625, 0.395751953125, 0.2872314453125, 0.415283203125, 0.317138671875, 0.3634033203125, 0.386474609375, 0.3988037109375, 0.4993896484375, 0.47314453125, 0.34228515625, 0.3751220703125, 0.431640625, 0.509765625, 0.4129638671875, 0.3651123046875, 0.51220703125, 0.4415283203125, 0.392333984375, 0.443115234375, 0.4136962890625, 0.5040359497070312, 0.4306640625, 0.54327392578125, 0.431884765625, 0.392578125, 0.4493560791015625, 0.457275390625, 0.3104248046875, 0.4981689453125, 0.415771484375, 0.5029296875, 0.42724609375, 0.3253173828125, 0.3798828125, 0.4986572265625, 0.3477783203125, 0.421875, 0.65625, 0.494049072265625, 0.2640380859375, 0.3966064453125, 0.3909454345703125, 0.505859375, 0.438720703125, 0.49945068359375, 0.6318359375, 0.372802734375, 0.361083984375, 0.567626953125, 0.44000244140625, 0.37158203125, 0.355224609375, 0.5701904296875, 0.384765625, 0.38330078125, 0.368896484375, 0.5, 0.48876953125, 0.3857421875, 0.3505706787109375, 0.5496063232421875, 0.384765625, 0.614959716796875, 0.399169921875, 0.30517578125, 0.291015625, 0.3837890625, 0.416259765625, 0.352783203125, 0.455810546875, 0.38348388671875, 0.352294921875, 0.390869140625, 0.4764404296875, 0.6458740234375, 0.840087890625, 0.385498046875, 0.50732421875, 0.3499755859375, 0.3834228515625, 0.3076171875, 0.39862060546875, 0.3662109375, 0.3892822265625, 0.4195556640625, 0.582275390625, 0.4376220703125, 0.55419921875, 0.362060546875, 0.37548828125, 0.383544921875, 0.3497314453125, 0.4300537109375, 0.444580078125, 0.33154296875, 0.39111328125, 0.50634765625, 0.35400390625, 0.489715576171875, 0.419189453125, 0.439453125, 0.3482666015625, 0.36962890625, 0.402099609375, 0.3095703125, 0.3812255859375, 0.491455078125, 0.4349365234375, 0.2984619140625, 0.4044189453125, 0.5816946029663086, 0.5728759765625, 0.3642578125, 0.553466796875, 0.4794921875, 0.49981689453125, 0.371826171875, 0.4833984375, 0.38214111328125, 0.44268798828125, 0.3748779296875, 0.534423828125, 0.41064453125, 0.4190673828125, 0.3798828125, 0.3408203125, 0.35693359375, 0.3236083984375, 0.4202880859375, 0.477783203125, 0.464599609375, 0.43505859375, 0.3580322265625, 0.549774169921875, 0.42431640625, 0.498046875, 0.487152099609375, 0.3837890625, 0.427978515625, 0.564453125, 0.4720458984375, 0.397705078125, 0.5670166015625, 0.4259033203125, 0.5384521484375, 0.5911865234375, 0.356201171875, 0.400634765625, 0.448486328125, 0.4490966796875, 0.4189453125, 0.339599609375, 0.3531494140625, 0.3974609375, 0.52459716796875, 0.45751953125, 0.5446014404296875, 0.589111328125, 0.41162109375, 0.431640625, 0.41845703125, 0.4610595703125, 0.373046875]\n",
      "Ground truth accuracy: 0.8859447240829468\n",
      "Unintended feature accuracy: 0.5011520981788635\n"
     ]
    }
   ],
   "source": [
    "cbp_probe, _ = train_probe(get_bottleneck, label_idx=0, dim=len(concepts), batches=get_data(batch_size=batch_size))\n",
    "batches = get_data(train=False, ambiguous=False, batch_size=batch_size)\n",
    "print('Ground truth accuracy:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.8610305190086365\n",
      "Accuracy for (0, 1): 0.8427337408065796\n",
      "Accuracy for (1, 0): 0.9193548560142517\n",
      "Accuracy for (1, 1): 0.928438663482666\n"
     ]
    }
   ],
   "source": [
    "# get subgroup accuracies\n",
    "subgroups = get_subgroups(train=False, ambiguous=False, batch_size=batch_size)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline neuron performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [48:11<00:00, 14.46s/it]\n"
     ]
    }
   ],
   "source": [
    "# get neurons which are most influential for giving gender label\n",
    "neuron_dicts = {\n",
    "    submodule : IdentityDict(activation_dim).to(DEVICE) for submodule in submodules\n",
    "}\n",
    "\n",
    "n_batches = 100\n",
    "batch_size = 1\n",
    "\n",
    "running_total = 0\n",
    "nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_text_batches(split=\"train\", ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        neuron_dicts,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if nodes is None:\n",
    "            nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, node in enumerate(nodes):\n",
    "    t.save(nodes[node], open(f\"effects_l0s_nearest_100/node_neuronskyline_{i}.pt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "331 1.90625\n",
      "535 6.90625\n",
      "540 2.015625\n",
      "1068 2.15625\n",
      "1142 4.125\n",
      "1393 6.4375\n",
      "Component 1:\n",
      "Component 2:\n",
      "Component 3:\n",
      "243 2.40625\n",
      "679 3.296875\n",
      "881 4.3125\n",
      "1170 2.1875\n",
      "1570 3.375\n",
      "2135 2.328125\n",
      "Component 4:\n",
      "Component 5:\n",
      "Component 6:\n",
      "1002 4.65625\n",
      "1570 3.640625\n",
      "2135 8.25\n",
      "Component 7:\n",
      "Component 8:\n",
      "Component 9:\n",
      "334 3.78125\n",
      "535 16.375\n",
      "1570 2.15625\n",
      "2135 6.71875\n",
      "Component 10:\n",
      "Component 11:\n",
      "Component 12:\n",
      "629 4.3125\n",
      "1570 4.46875\n",
      "Component 13:\n",
      "Component 14:\n",
      "Component 15:\n",
      "482 2.234375\n",
      "1068 2.140625\n",
      "1261 2.09375\n",
      "1570 4.25\n",
      "Component 16:\n",
      "Component 17:\n",
      "Component 18:\n",
      "624 2.65625\n",
      "682 2.375\n",
      "1068 2.203125\n",
      "1408 3.1875\n",
      "1570 4.1875\n",
      "Component 19:\n",
      "Component 20:\n",
      "Component 21:\n",
      "292 1.8515625\n",
      "624 5.875\n",
      "682 4.53125\n",
      "1068 4.8125\n",
      "1570 3.78125\n",
      "Component 22:\n",
      "Component 23:\n",
      "Component 24:\n",
      "535 17.25\n",
      "1261 1.8671875\n",
      "1699 1.9921875\n",
      "Component 25:\n",
      "Component 26:\n",
      "1340 1.859375\n",
      "Component 27:\n",
      "1699 2.375\n",
      "Component 28:\n",
      "Component 29:\n",
      "Component 30:\n",
      "682 4.375\n",
      "1068 2.78125\n",
      "1170 2.421875\n",
      "Component 31:\n",
      "Component 32:\n",
      "Component 33:\n",
      "334 6.15625\n",
      "535 6.1875\n",
      "Component 34:\n",
      "Component 35:\n",
      "Component 36:\n",
      "535 6.625\n",
      "1068 2.046875\n",
      "1592 6.15625\n",
      "1704 1.9296875\n",
      "1807 3.265625\n",
      "2169 5.28125\n",
      "Component 37:\n",
      "Component 38:\n",
      "Component 39:\n",
      "682 12.3125\n",
      "784 2.59375\n",
      "1068 8.375\n",
      "1570 7.40625\n",
      "1807 3.984375\n",
      "Component 40:\n",
      "Component 41:\n",
      "Component 42:\n",
      "482 3.34375\n",
      "682 3.296875\n",
      "1068 3.296875\n",
      "1570 3.78125\n",
      "Component 43:\n",
      "Component 44:\n",
      "Component 45:\n",
      "Component 46:\n",
      "Component 47:\n",
      "Component 48:\n",
      "405 2.484375\n",
      "682 4.25\n",
      "1068 4.03125\n",
      "1645 2.265625\n",
      "1807 2.203125\n",
      "Component 49:\n",
      "Component 50:\n",
      "Component 51:\n",
      "535 2.203125\n",
      "682 4.5625\n",
      "1068 5.90625\n",
      "1408 2.171875\n",
      "1570 2.171875\n",
      "1645 2.765625\n",
      "1711 2.21875\n",
      "Component 52:\n",
      "Component 53:\n",
      "Component 54:\n",
      "98 2.21875\n",
      "113 2.484375\n",
      "682 10.1875\n",
      "784 2.625\n",
      "1068 16.375\n",
      "1149 3.1875\n",
      "1546 5.6875\n",
      "1570 4.6875\n",
      "1645 6.8125\n",
      "1699 1.8203125\n",
      "1807 3.515625\n",
      "2106 2.140625\n",
      "Component 55:\n",
      "Component 56:\n",
      "Component 57:\n",
      "Component 58:\n",
      "Component 59:\n",
      "Component 60:\n",
      "total neurons: 84\n"
     ]
    }
   ],
   "source": [
    "def n_hot(feats, dim=2304):\n",
    "    out = t.zeros(dim, dtype=t.bool, device=DEVICE)\n",
    "    for feat in feats:\n",
    "        out[feat] = True\n",
    "    return out\n",
    "\n",
    "neurons_to_ablate = {}\n",
    "total_neurons = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    neurons_to_ablate[submodules[component_idx]] = []\n",
    "    if effect.act.shape[-1] != 2304:\n",
    "        continue\n",
    "    for idx in (effect.act > 1.8).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        neurons_to_ablate[submodules[component_idx]].append(idx.item())\n",
    "        total_neurons += 1\n",
    "print(f\"total neurons: {total_neurons}\")\n",
    "\n",
    "neurons_to_ablate = {\n",
    "    submodule : n_hot([neuron_idx], dim=2304) for submodule, neuron_idx in neurons_to_ablate.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_text_batches = get_text_batches(split=\"test\", ambiguous=True, batch_size=batch_size)\n",
    "print(\n",
    "    'Ambiguous test accuracy:', \n",
    "    test_probe(\n",
    "        probe,\n",
    "        activation_batches=collect_acts_ablated(ambiguous_text_batches, model, submodules, neuron_dicts, neurons_to_ablate, layer),\n",
    "    )\n",
    ")\n",
    "unambiguous_text_batches = get_text_batches(split=\"test\", ambiguous=False, batch_size=batch_size)\n",
    "print(\n",
    "    \"Ground truth accuracy:\",\n",
    "    test_probe(\n",
    "        probe,\n",
    "        activation_batches=collect_acts_ablated(unambiguous_text_batches, model, submodules, neuron_dicts, neurons_to_ablate, layer),\n",
    "        label_idx=0,\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    \"Spurious accuracy:\",\n",
    "    test_probe(\n",
    "        probe,\n",
    "        activation_batches=collect_acts_ablated(unambiguous_text_batches, model, submodules, neuron_dicts, neurons_to_ablate, layer),\n",
    "        label_idx=1,\n",
    "    ),\n",
    ")\n",
    "subgroups = get_subgroups(split=\"test\", ambiguous=False, batch_size=batch_size)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(\n",
    "        f\"Accuracy for {label_profile}:\",\n",
    "        test_probe(\n",
    "            probe,\n",
    "            activation_batches=collect_acts_ablated(batches, model, submodules, neuron_dicts, neurons_to_ablate, layer),\n",
    "            label_idx=0,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline feature performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [18:59<00:00, 11.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# get features which are most useful for predicting gender label\n",
    "n_batches = 100\n",
    "batch_size = 1\n",
    "\n",
    "running_total = 0\n",
    "running_nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_text_batches(split=\"train\", ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    hash_input = clean + [s.name for s in submodules]\n",
    "    hash_str = ''.join(hash_input) + \"_feature_skyline\"\n",
    "    hash_digest = hashlib.md5(hash_str.encode()).hexdigest()\n",
    "    if os.path.exists(f\"effects/{hash_digest}.pt\"):\n",
    "        continue\n",
    "\n",
    "    effects, *_ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    to_save = {\n",
    "        k.name : v.detach().to(\"cpu\") for k, v in effects.items()\n",
    "    }\n",
    "    t.save(to_save, f\"effects/{hash_digest}.pt\")\n",
    "\n",
    "    del effects\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate effects\n",
    "aggregated_effects = {submodule.name : 0 for submodule in submodules}\n",
    "\n",
    "for idx, (clean, *_) in enumerate(get_text_batches(split=\"train\", ambiguous=False, batch_size=batch_size)):\n",
    "    if idx == n_batches:\n",
    "        break\n",
    "    hash_input = clean + [s.name for s in submodules]\n",
    "    hash_str = ''.join(hash_input) + \"_feature_skyline\"\n",
    "    hash_digest = hashlib.md5(hash_str.encode()).hexdigest()\n",
    "    effects = t.load(f\"effects/{hash_digest}.pt\")\n",
    "    for submodule in submodules:\n",
    "        aggregated_effects[submodule.name] += (\n",
    "            effects[submodule.name].act[:,1:,:] # ignore BOS token\n",
    "        ).sum(dim=1).mean(dim=0)\n",
    "\n",
    "aggregated_effects = {k : v / (batch_size * n_batches) for k, v in aggregated_effects.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component attn_0:\n",
      "Component mlp_0:\n",
      "Component resid_0:\n",
      "4449 8.0\n",
      "Component attn_1:\n",
      "Component mlp_1:\n",
      "Component resid_1:\n",
      "11782 6.96875\n",
      "Component attn_2:\n",
      "Component mlp_2:\n",
      "Component resid_2:\n",
      "5853 6.53125\n",
      "12818 6.1875\n",
      "Component attn_3:\n",
      "Component mlp_3:\n",
      "Component resid_3:\n",
      "9949 7.0\n",
      "Component attn_4:\n",
      "Component mlp_4:\n",
      "Component resid_4:\n",
      "4675 6.53125\n",
      "Component attn_5:\n",
      "Component mlp_5:\n",
      "Component resid_5:\n",
      "2864 9.8125\n",
      "11682 7.125\n",
      "Component attn_6:\n",
      "Component mlp_6:\n",
      "Component resid_6:\n",
      "4068 9.3125\n",
      "7008 9.625\n",
      "Component attn_7:\n",
      "Component mlp_7:\n",
      "Component resid_7:\n",
      "7111 6.84375\n",
      "Component attn_8:\n",
      "Component mlp_8:\n",
      "Component resid_8:\n",
      "6952 7.875\n",
      "9949 14.625\n",
      "Component attn_9:\n",
      "Component mlp_9:\n",
      "Component resid_9:\n",
      "6952 10.0\n",
      "15246 11.25\n",
      "Component attn_10:\n",
      "Component mlp_10:\n",
      "Component resid_10:\n",
      "3711 11.1875\n",
      "6952 10.25\n",
      "Component attn_11:\n",
      "Component mlp_11:\n",
      "Component resid_11:\n",
      "3013 19.75\n",
      "4467 8.1875\n",
      "11649 6.15625\n",
      "Component attn_12:\n",
      "Component mlp_12:\n",
      "Component resid_12:\n",
      "6335 10.1875\n",
      "11114 25.75\n",
      "Component attn_13:\n",
      "Component mlp_13:\n",
      "Component resid_13:\n",
      "192 25.875\n",
      "14755 7.03125\n",
      "Component attn_14:\n",
      "Component mlp_14:\n",
      "Component resid_14:\n",
      "2354 22.375\n",
      "12665 6.46875\n",
      "Component attn_15:\n",
      "Component mlp_15:\n",
      "Component resid_15:\n",
      "798 19.625\n",
      "6211 7.15625\n",
      "Component attn_16:\n",
      "Component mlp_16:\n",
      "Component resid_16:\n",
      "15567 6.625\n",
      "16351 20.625\n",
      "Component attn_17:\n",
      "Component mlp_17:\n",
      "Component resid_17:\n",
      "6011 14.5625\n",
      "6257 6.125\n",
      "Component attn_18:\n",
      "Component mlp_18:\n",
      "Component resid_18:\n",
      "61 10.9375\n",
      "Component attn_19:\n",
      "Component mlp_19:\n",
      "Component resid_19:\n",
      "13002 9.3125\n",
      "Component attn_20:\n",
      "9711 6.4375\n",
      "Component mlp_20:\n",
      "Component resid_20:\n",
      "7116 10.3125\n",
      "12545 14.5625\n",
      "Component attn_21:\n",
      "2740 10.8125\n",
      "10118 8.25\n",
      "Component mlp_21:\n",
      "Component resid_21:\n",
      "1065 14.5\n",
      "4430 20.375\n",
      "Component attn_22:\n",
      "Component mlp_22:\n",
      "Component resid_22:\n",
      "1208 16.5\n",
      "3497 21.0\n",
      "total features: 43\n"
     ]
    }
   ],
   "source": [
    "top_feats_to_ablate = {}\n",
    "total_features = 0\n",
    "for submodule in submodules:\n",
    "    print(f\"Component {submodule.name}:\")\n",
    "    top_feats_to_ablate[submodule.name] = []\n",
    "    for idx in (aggregated_effects[submodule.name] > 6.1).nonzero():\n",
    "        print(idx.item(), aggregated_effects[submodule.name][idx].item())\n",
    "        top_feats_to_ablate[submodule.name].append(idx.item())\n",
    "        total_features += 1\n",
    "print(f\"total features: {total_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_hot(feats, dim):\n",
    "    out = t.zeros(dim, dtype=t.bool, device=DEVICE)\n",
    "    for feat in feats:\n",
    "        out[feat] = True\n",
    "    return out\n",
    "\n",
    "top_feats_to_ablate = {\n",
    "    submodule : n_hot(top_feats_to_ablate[submodule.name], dictionaries[submodule].dict_size) for submodule in submodules\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations with ablations: 100%|██████████| 269/269 [02:32<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.8300418257713318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations with ablations: 100%|██████████| 55/55 [00:30<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth accuracy: 0.8087557554244995\n",
      "Spurious accuracy: 0.5368663668632507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ambiguous_accs = test_probe(\n",
    "    probe,\n",
    "    activation_batches=collect_acts_ablated(\n",
    "        get_text_batches(split=\"test\", ambiguous=True), \n",
    "        model, submodules, dictionaries, top_feats_to_ablate, layer\n",
    "    ),\n",
    ")\n",
    "print(f\"Ambiguous test accuracy: {ambiguous_accs[0]}\")\n",
    "\n",
    "unambiguous_accs = test_probe(\n",
    "    probe,\n",
    "    activation_batches=collect_acts_ablated(\n",
    "        get_text_batches(split=\"test\", ambiguous=False),\n",
    "        model, submodules, dictionaries, top_feats_to_ablate, layer\n",
    "    ),\n",
    ")\n",
    "print(f\"Ground truth accuracy: {unambiguous_accs[0]}\")\n",
    "print(f\"Spurious accuracy: {unambiguous_accs[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining probe on activations after ablating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations with ablations: 100%|██████████| 700/700 [06:59<00:00,  1.67it/s]\n",
      "Collecting activations with ablations: 100%|██████████| 269/269 [02:40<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.977811336517334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations with ablations: 100%|██████████| 55/55 [00:31<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth accuracy: 0.9504608511924744\n",
      "Spurious accuracy: 0.524193525314331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retrained_probe, _ = train_probe(\n",
    "    activation_batches=collect_acts_ablated(\n",
    "        get_text_batches(split=\"train\", ambiguous=True), \n",
    "        model, submodules, dictionaries, feats_to_ablate, layer\n",
    "    ),\n",
    ")\n",
    "ambiguous_test_accs = test_probe(\n",
    "    retrained_probe,\n",
    "    activation_batches=collect_acts_ablated(\n",
    "        get_text_batches(split=\"test\", ambiguous=True),\n",
    "        model, submodules, dictionaries, feats_to_ablate, layer\n",
    "    ),\n",
    ")\n",
    "print(f\"Ambiguous test accuracy: {ambiguous_test_accs[0]}\")\n",
    "unambiguous_test_accs = test_probe(\n",
    "    retrained_probe,\n",
    "    activation_batches=collect_acts_ablated(\n",
    "        get_text_batches(split=\"test\", ambiguous=False),\n",
    "        model, submodules, dictionaries, feats_to_ablate, layer\n",
    "    ),\n",
    ")\n",
    "print(f\"Ground truth accuracy: {unambiguous_test_accs[0]}\")\n",
    "print(f\"Spurious accuracy: {unambiguous_test_accs[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
